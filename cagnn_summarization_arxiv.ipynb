{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7ec4f584",
      "metadata": {
        "collapsed": false,
        "id": "7ec4f584"
      },
      "source": [
        "# Dataset\n",
        "[Dataset Link](https://huggingface.co/datasets/ccdv/arxiv-summarization?utm_source=chatgpt.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceccd515",
      "metadata": {
        "id": "ceccd515"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets transformers spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c23c8e",
      "metadata": {
        "collapsed": false,
        "id": "17c23c8e"
      },
      "source": [
        "# Load Dataset and Inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe10028",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bfe10028"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset (train split only for now)\n",
        "dataset = load_dataset(\"ccdv/arxiv-summarization\", split=\"train\")\n",
        "\n",
        "# View the structure of the first sample\n",
        "print(dataset[0].keys())\n",
        "print(\"Sample Article:\", dataset[0]['article'][:500])  # Preview only\n",
        "print(\"Sample Abstract:\", dataset[0]['abstract'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6b57a20",
      "metadata": {
        "collapsed": false,
        "id": "f6b57a20"
      },
      "source": [
        "# Sentence Segmentation & Named Entity Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca802a86",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ca802a86"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for sentence splitting and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load once globally\n",
        "nlp_ner = spacy.load(\"en_core_web_sm\")\n",
        "nlp_sent = spacy.blank(\"en\")\n",
        "nlp_sent.add_pipe(\"sentencizer\")\n",
        "\n",
        "def preprocess_article(text, max_chars=50000, use_entities=True):\n",
        "    text = text[:max_chars]\n",
        "\n",
        "    if use_entities:\n",
        "        doc = next(nlp_ner.pipe([text]))\n",
        "        entities = list(set(\n",
        "            ent.text.strip()\n",
        "            for ent in doc.ents\n",
        "            if ent.label_ in {'PERSON', 'ORG', 'GPE', 'DATE', 'WORK_OF_ART'}\n",
        "        ))\n",
        "    else:\n",
        "        doc = next(nlp_sent.pipe([text]))\n",
        "        entities = []\n",
        "\n",
        "    # Filter better sentences\n",
        "    sentences = [\n",
        "        sent.text.strip() for sent in doc.sents\n",
        "        if len(sent.text.strip()) > 40 and not sent.text.strip().startswith('*') and not sent.text.strip().isdigit()\n",
        "    ]\n",
        "\n",
        "    return sentences, entities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "693a840e",
      "metadata": {
        "collapsed": false,
        "id": "693a840e"
      },
      "source": [
        "# Graph Construction with DGL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2f6d98",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5e2f6d98"
      },
      "outputs": [],
      "source": [
        "pip install -q torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64db217",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e64db217"
      },
      "outputs": [],
      "source": [
        "pip install -q dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19aa02fd",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "19aa02fd"
      },
      "outputs": [],
      "source": [
        "import dgl\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8252f3",
      "metadata": {
        "collapsed": false,
        "id": "5d8252f3"
      },
      "source": [
        "## build graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6dbdbe4",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a6dbdbe4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "883de97f",
      "metadata": {
        "collapsed": false,
        "id": "883de97f"
      },
      "source": [
        "## Build and Inspect the Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bde875c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5bde875c"
      },
      "outputs": [],
      "source": [
        "# !pip install sentence-transformers==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611d5374",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "611d5374"
      },
      "outputs": [],
      "source": [
        "# # Install accelerate\n",
        "!pip install -q accelerate==0.29.3\n",
        "\n",
        "# # Install transformers\n",
        "!pip install -q transformers==4.39.3\n",
        "\n",
        "# # Install sentence-transformers\n",
        "# !pip install sentence-transformers==2.2.2\n",
        "\n",
        "# # Install bertopic\n",
        "# !pip install bertopic==0.16.0\n",
        "\n",
        "# # Ensure spaCy and its model are installed/downloaded\n",
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b68ea75d",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b68ea75d"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1f31355",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f1f31355"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers==2.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00aec8c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d00aec8c"
      },
      "outputs": [],
      "source": [
        "!pip install -q bertopic==0.16.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4592aec3",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4592aec3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Device Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap # Import UMAP for custom model\n",
        "import hdbscan # Import HDBSCAN for custom model (good practice for BERTopic)\n",
        "import numpy as np # For potential min/max operations\n",
        "\n",
        "# --- NEW: Prepare a larger collection of sentences for BERTopic ---\n",
        "all_sentences_for_bertopic = []\n",
        "num_articles_to_process = 200 # Adjust this number!\n",
        "\n",
        "print(f\"Collecting sentences from {num_articles_to_process} articles...\")\n",
        "for i in range(num_articles_to_process):\n",
        "    if i >= len(dataset):\n",
        "        print(f\"Reached end of dataset at article {i}. Stopping collection.\")\n",
        "        break\n",
        "    if i % 100 == 0 and i != 0: # Print progress every 100 articles after the first\n",
        "        print(f\"Processing article {i}...\")\n",
        "    article_text = dataset[i]['article']\n",
        "    sents, _ = preprocess_article(article_text)\n",
        "    all_sentences_for_bertopic.extend(sents)\n",
        "\n",
        "print(f\"Total sentences collected for BERTopic: {len(all_sentences_for_bertopic)}\")\n",
        "\n",
        "if len(all_sentences_for_bertopic) < 50:\n",
        "    print(\"WARNING: Insufficient sentences collected for meaningful topic modeling. Consider increasing `num_articles_to_process`.\")\n",
        "\n",
        "# --- Rest of your BERTopic code, now using `all_sentences_for_bertopic` ---\n",
        "\n",
        "# Embed the sentences\n",
        "# SentenceTransformer will automatically use GPU if available and properly configured with PyTorch-CUDA\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(\"Embedding sentences (this might take a while for large datasets)...\")\n",
        "# The encoding process will now run on GPU if available\n",
        "# sentence_embeddings = sbert_model.encode(all_sentences_for_bertopic, show_progress_bar=True)\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 256\n",
        "sentence_embeddings = []\n",
        "for i in tqdm(range(0, len(all_sentences_for_bertopic), batch_size)):\n",
        "    batch = all_sentences_for_bertopic[i:i+batch_size]\n",
        "    sentence_embeddings.extend(sbert_model.encode(batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6c2e05",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2a6c2e05"
      },
      "outputs": [],
      "source": [
        "# Configure UMAP and HDBSCAN for BERTopic\n",
        "umap_n_neighbors = max(2, min(15, len(all_sentences_for_bertopic) - 1))\n",
        "umap_model = umap.UMAP(n_neighbors=umap_n_neighbors,\n",
        "                       n_components=5,\n",
        "                       min_dist=0.0,\n",
        "                       random_state=42)\n",
        "\n",
        "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=50,\n",
        "                                metric='euclidean',\n",
        "                                prediction_data=True)\n",
        "\n",
        "print(\"Fitting BERTopic model (this might take a while)...\")\n",
        "topic_model = BERTopic(embedding_model=sbert_model,\n",
        "                       umap_model=umap_model,\n",
        "                       hdbscan_model=hdbscan_model,\n",
        "                       calculate_probabilities=True,\n",
        "                       verbose=True)\n",
        "\n",
        "# topics, probs = topic_model.fit_transform(all_sentences_for_bertopic, embeddings=sentence_embeddings)\n",
        "import numpy as np\n",
        "\n",
        "sentence_embeddings = np.array(sentence_embeddings)  # Convert list to array\n",
        "topics, probs = topic_model.fit_transform(all_sentences_for_bertopic, embeddings=sentence_embeddings)\n",
        "\n",
        "print(\"\\nBERTopic ran successfully!\")\n",
        "print(f\"Number of topics found: {len(topic_model.get_topic_info()) - 1}\")\n",
        "\n",
        "print(\"\\nTop 10 Topics:\")\n",
        "print(topic_model.get_topic_info().head(11))\n",
        "\n",
        "if 0 in topic_model.get_topic_info()['Topic'].values:\n",
        "    print(\"\\nWords for Topic 0:\")\n",
        "    print(topic_model.get_topic(0))\n",
        "else:\n",
        "    print(\"\\nTopic 0 does not exist (possibly all data are outliers or no clear topics).\")\n",
        "\n",
        "# Optional: Visualize topics if you have enough data and want to confirm\n",
        "# from bertopic import plotting\n",
        "# if len(topics) > 1:\n",
        "#    fig = topic_model.visualize_topics()\n",
        "#    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d83953c3",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d83953c3"
      },
      "outputs": [],
      "source": [
        "from bertopic import plotting\n",
        "if len(topics) > 1: # Only visualize if more than just the outlier topic\n",
        "    fig = topic_model.visualize_topics()\n",
        "    fig.show()\n",
        "\n",
        "    # Other useful visualizations:\n",
        "    fig = topic_model.visualize_barchart(top_n_topics=10)\n",
        "    fig.show()\n",
        "    fig = topic_model.visualize_heatmap()\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e05d92a",
      "metadata": {
        "collapsed": false,
        "id": "2e05d92a"
      },
      "source": [
        "# Add topic nodes and sentence-to-topic edges"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e03e361b",
      "metadata": {
        "collapsed": false,
        "id": "e03e361b"
      },
      "source": [
        "# corefers_with edges between sentence nodes and discourse_follows edges (sequential sentence connections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ad95ea",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "23ad95ea"
      },
      "outputs": [],
      "source": [
        "# neuralcoref was built for spaCy 2.x, we can still use a fork that works for spaCy 3.x\n",
        "!pip install git+https://github.com/huggingface/neuralcoref.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8950b21b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8950b21b"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# import coreferee\n",
        "\n",
        "# # Load coreference model (do once)\n",
        "# nlp = spacy.load(\"en_coreference_web_trf\")\n",
        "# nlp.add_pipe(\"coreferee\")\n",
        "\n",
        "# def build_graph_with_coref_and_discourse(sentences, entities, topic_ids=None):\n",
        "#     import torch\n",
        "#     import dgl\n",
        "#     from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#     from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#     num_sents = len(sentences)\n",
        "#     num_ents = len(entities)\n",
        "#     num_topics = len(set(topic_ids)) if topic_ids else 0\n",
        "\n",
        "#     # --- Edge containers\n",
        "#     edge_dict = {\n",
        "#         ('sentence', 'mentions', 'entity'): ([], []),\n",
        "#         ('sentence', 'belongs_to', 'topic'): ([], []),\n",
        "#         ('sentence', 'corefers_with', 'sentence'): ([], []),\n",
        "#         ('sentence', 'discourse_follows', 'sentence'): ([], [])\n",
        "#     }\n",
        "\n",
        "#     # --- Sentence-to-entity edges\n",
        "#     for s_idx, sent in enumerate(sentences):\n",
        "#         for e_idx, ent in enumerate(entities):\n",
        "#             if ent.lower() in sent.lower():\n",
        "#                 edge_dict[('sentence', 'mentions', 'entity')][0].append(s_idx)\n",
        "#                 edge_dict[('sentence', 'mentions', 'entity')][1].append(e_idx)\n",
        "\n",
        "#     # --- Sentence-to-topic edges\n",
        "#     if topic_ids:\n",
        "#         topic_map = {t: i for i, t in enumerate(sorted(set(topic_ids)))}\n",
        "#         for s_idx, t_id in enumerate(topic_ids):\n",
        "#             if t_id != -1:\n",
        "#                 topic_idx = topic_map[t_id]\n",
        "#                 edge_dict[('sentence', 'belongs_to', 'topic')][0].append(s_idx)\n",
        "#                 edge_dict[('sentence', 'belongs_to', 'topic')][1].append(topic_idx)\n",
        "\n",
        "#     # --- Co-reference edges\n",
        "#     doc = nlp(\" \".join(sentences))\n",
        "#     token_to_sent = {token.i: i for i, s in enumerate(doc.sents) for token in s}\n",
        "\n",
        "#     if doc._.has_coref:\n",
        "#         for chain in doc._.coref_chains:\n",
        "#             mentions = chain.get_mentions()\n",
        "#             sent_indices = list(set(\n",
        "#                 token_to_sent.get(m.start)\n",
        "#                 for m in mentions if token_to_sent.get(m.start) is not None\n",
        "#             ))\n",
        "#             for i in range(len(sent_indices)):\n",
        "#                 for j in range(i + 1, len(sent_indices)):\n",
        "#                     a, b = sent_indices[i], sent_indices[j]\n",
        "#                     edge_dict[('sentence', 'corefers_with', 'sentence')][0] += [a, b]\n",
        "#                     edge_dict[('sentence', 'corefers_with', 'sentence')][1] += [b, a]\n",
        "\n",
        "#     # --- Discourse edges (sequential)\n",
        "#     for i in range(num_sents - 1):\n",
        "#         edge_dict[('sentence', 'discourse_follows', 'sentence')][0] += [i, i + 1]\n",
        "#         edge_dict[('sentence', 'discourse_follows', 'sentence')][1] += [i + 1, i]\n",
        "\n",
        "#     # --- Graph init\n",
        "#     node_dict = {'sentence': num_sents, 'entity': num_ents}\n",
        "#     if topic_ids:\n",
        "#         node_dict['topic'] = num_topics\n",
        "\n",
        "#     # Filter empty edge types\n",
        "#     graph_data = {\n",
        "#         etype: (torch.tensor(src), torch.tensor(dst))\n",
        "#         for etype, (src, dst) in edge_dict.items()\n",
        "#         if src and dst\n",
        "#     }\n",
        "\n",
        "#     g = dgl.heterograph(graph_data, num_nodes_dict=node_dict)\n",
        "#     return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45a1fb4",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e45a1fb4"
      },
      "outputs": [],
      "source": [
        "def build_graph_with_topics(sentences, entities, precomputed_topics=None):\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import torch\n",
        "    import dgl\n",
        "\n",
        "    # TF-IDF for sentence similarity\n",
        "    vectorizer = TfidfVectorizer(max_features=300)\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    num_sentences = len(sentences)\n",
        "    num_entities = len(entities)\n",
        "\n",
        "    sentence_sim_src = []\n",
        "    sentence_sim_dst = []\n",
        "    sent_ent_src = []\n",
        "    sent_ent_dst = []\n",
        "\n",
        "    # Sentence-to-sentence edges\n",
        "    threshold = 0.2\n",
        "    for i in range(num_sentences):\n",
        "        for j in range(num_sentences):\n",
        "            if i != j and sim_matrix[i][j] > threshold:\n",
        "                sentence_sim_src.append(i)\n",
        "                sentence_sim_dst.append(j)\n",
        "\n",
        "    # Sentence-to-entity edges\n",
        "    for sent_idx, sentence in enumerate(sentences):\n",
        "        for ent_idx, entity in enumerate(entities):\n",
        "            if entity in sentence:\n",
        "                sent_ent_src.append(sent_idx)\n",
        "                sent_ent_dst.append(num_sentences + ent_idx)\n",
        "\n",
        "    graph_data = {}\n",
        "\n",
        "    if sentence_sim_src:\n",
        "        graph_data[('sentence', 'similar_to', 'sentence')] = (\n",
        "            torch.tensor(sentence_sim_src, dtype=torch.int32),\n",
        "            torch.tensor(sentence_sim_dst, dtype=torch.int32)\n",
        "        )\n",
        "\n",
        "    if sent_ent_src:\n",
        "        graph_data[('sentence', 'mentions', 'entity')] = (\n",
        "            torch.tensor(sent_ent_src, dtype=torch.int32),\n",
        "            torch.tensor(sent_ent_dst, dtype=torch.int32)\n",
        "        )\n",
        "\n",
        "    # Sentence-to-topic edges\n",
        "    sent_topic_src = []\n",
        "    sent_topic_dst = []\n",
        "    topic_set = []\n",
        "    topic_id_map = {}\n",
        "\n",
        "    if precomputed_topics is not None:\n",
        "        topic_set = sorted(set(t for t in precomputed_topics if t != -1))\n",
        "        topic_id_map = {topic_id: idx for idx, topic_id in enumerate(topic_set)}\n",
        "        for sent_idx, topic_id in enumerate(precomputed_topics):\n",
        "            if topic_id != -1:\n",
        "                topic_idx = topic_id_map[topic_id]\n",
        "                sent_topic_src.append(sent_idx)\n",
        "                sent_topic_dst.append(num_sentences + num_entities + topic_idx)\n",
        "\n",
        "        if sent_topic_src:\n",
        "            graph_data[('sentence', 'belongs_to', 'topic')] = (\n",
        "                torch.tensor(sent_topic_src, dtype=torch.int32),\n",
        "                torch.tensor(sent_topic_dst, dtype=torch.int32)\n",
        "            )\n",
        "\n",
        "    print(f\"Sent-Sent edges: {len(sentence_sim_src)}\")\n",
        "    print(f\"Sent-Ent edges: {len(sent_ent_src)}\")\n",
        "    print(f\"Sent-Topic edges: {len(sent_topic_src)}\")\n",
        "    print(f\"Unique topics found: {len(topic_set)}\")\n",
        "\n",
        "    if not graph_data:\n",
        "        raise ValueError(\"Graph has no edges.\")\n",
        "\n",
        "    g = dgl.heterograph(graph_data)\n",
        "\n",
        "    return g, sentences, entities, topic_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae7e7fe",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aae7e7fe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26043edc",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "26043edc"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Combine multiple articles for better topic matching\n",
        "combined_text = \"\\n\".join(dataset[i]['article'] for i in range(200))  # ‚Üê try 20 or even 50\n",
        "\n",
        "# ‚úÖ Preprocess the combined article text\n",
        "sentences, entities = preprocess_article(combined_text)\n",
        "\n",
        "# ‚úÖ Get topic assignments using the trained BERTopic model\n",
        "document_topics, _ = topic_model.transform(sentences)\n",
        "\n",
        "# ‚úÖ Build the heterogeneous graph with sentence-entity-topic structure\n",
        "graph, sent_nodes, ent_nodes, topic_ids = build_graph_with_topics(\n",
        "    sentences,\n",
        "    entities,\n",
        "    precomputed_topics=document_topics\n",
        ")\n",
        "\n",
        "# ‚úÖ Inspect graph structure\n",
        "print(\"\\n--- Graph Structure ---\")\n",
        "print(graph)\n",
        "print(\"Node types:\", graph.ntypes)\n",
        "print(\"Edge types:\", graph.etypes)\n",
        "\n",
        "# ‚úÖ Optional: check sent-topic edge count\n",
        "if ('sentence', 'belongs_to', 'topic') in graph.canonical_etypes:\n",
        "    print(f\"‚úÖ Sent-Topic edges: {graph.num_edges(('sentence', 'belongs_to', 'topic'))}\")\n",
        "else:\n",
        "    print(\"‚ùå No Sent-Topic edges created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb07c60d",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bb07c60d"
      },
      "outputs": [],
      "source": [
        "print(\"Assigned topics:\", set(document_topics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e98c34b8",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e98c34b8"
      },
      "outputs": [],
      "source": [
        "# topics, probs = topic_model.fit_transform(sentences)\n",
        "# Transform individual document using global BERTopic\n",
        "document_topics, _ = topic_model.transform(sentences)\n",
        "\n",
        "print(\"Topics assigned:\", topics)\n",
        "print(\"Unique topics (excluding -1):\", set(t for t in topics if t != -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e844ef",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "49e844ef"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"bertopic_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(topic_model, f)\n",
        "\n",
        "np.save(\"sentence_embeddings.npy\", sentence_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44f56f88",
      "metadata": {
        "collapsed": false,
        "id": "44f56f88"
      },
      "source": [
        "# Graph Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f0bfc5b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1f0bfc5b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "from dgl.nn import HeteroGraphConv, GraphConv\n",
        "\n",
        "# Graph Encoder for Heterogeneous Graph\n",
        "class HeteroGraphEncoder(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats, out_feats, rel_names):\n",
        "        super().__init__()\n",
        "        # Define two-layer HeteroGraphConv\n",
        "        self.layer1 = HeteroGraphConv({\n",
        "            rel: GraphConv(in_feats, hidden_feats)\n",
        "            for rel in rel_names\n",
        "        }, aggregate='mean')\n",
        "\n",
        "        self.layer2 = HeteroGraphConv({\n",
        "            rel: GraphConv(hidden_feats, out_feats)\n",
        "            for rel in rel_names\n",
        "        }, aggregate='mean')\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        # inputs is a dictionary of {ntype: feature_tensor}\n",
        "        h = self.layer1(g, inputs)\n",
        "        h = {k: F.relu(v) for k, v in h.items()}\n",
        "        h = self.layer2(g, h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c8b9d51",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2c8b9d51"
      },
      "outputs": [],
      "source": [
        "# 1. Define input features (e.g., use TF-IDF vectors or random init)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=300)\n",
        "sentence_feats = vectorizer.fit_transform(sent_nodes).toarray()\n",
        "sentence_feats = torch.tensor(sentence_feats, dtype=torch.float32)\n",
        "\n",
        "# Optional: init entity/topic features as zeros or random\n",
        "entity_feats = torch.randn(graph.num_nodes('entity'), 300)\n",
        "topic_feats = torch.randn(graph.num_nodes('topic'), 300)\n",
        "\n",
        "# 2. Combine into dictionary\n",
        "features = {\n",
        "    'sentence': sentence_feats,\n",
        "    'entity': entity_feats,\n",
        "    'topic': topic_feats\n",
        "}\n",
        "\n",
        "# 3. Instantiate the model\n",
        "rel_names = list(graph.etypes)\n",
        "model = HeteroGraphEncoder(in_feats=300, hidden_feats=128, out_feats=64, rel_names=rel_names)\n",
        "\n",
        "# 4. Forward pass to get updated node embeddings\n",
        "model.eval()  # Disable dropout/batchnorm for inference\n",
        "with torch.no_grad():\n",
        "    node_embeddings = model(graph, features)\n",
        "\n",
        "# 5. Extract sentence node embeddings for summary ranking\n",
        "sentence_embeddings = node_embeddings['sentence']\n",
        "print(\"Sentence Embeddings Shape:\", sentence_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "103cfade",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "103cfade"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class MultiLevelGraphAttentionPooling(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.sentence_attn = nn.Linear(embed_dim, 1)\n",
        "        self.entity_attn = nn.Linear(embed_dim, 1)\n",
        "        self.topic_attn = nn.Linear(embed_dim, 1)\n",
        "\n",
        "        self.entity_proj = nn.Linear(300, embed_dim)\n",
        "        self.topic_proj = nn.Linear(300, embed_dim)\n",
        "\n",
        "        # Learnable fusion weights\n",
        "        self.raw_weights = nn.Parameter(torch.tensor([1.0, 1.0, 1.0]))  # [Œ±, Œ≤, Œ≥]\n",
        "\n",
        "    def dense_adj_from_edges(self, g, etype, src_size, dst_size):\n",
        "        src, dst = g.edges(etype=etype)\n",
        "        adj = torch.zeros((src_size, dst_size), device=src.device)\n",
        "        adj[src, dst] = 1\n",
        "        return adj\n",
        "\n",
        "    def forward(self, sentence_embs, entity_embs, topic_embs, graph, attention_breakdown=False):\n",
        "        sent_scores = self.sentence_attn(sentence_embs).squeeze(1)\n",
        "\n",
        "        entity_embs_proj = self.entity_proj(entity_embs)\n",
        "        topic_embs_proj = self.topic_proj(topic_embs)\n",
        "\n",
        "        # Entity attention\n",
        "        if ('sentence', 'mentions', 'entity') in graph.canonical_etypes:\n",
        "            adj_se = self.dense_adj_from_edges(graph, ('sentence', 'mentions', 'entity'),\n",
        "                                               graph.num_nodes('sentence'), graph.num_nodes('entity'))\n",
        "            ent_context = adj_se @ entity_embs_proj\n",
        "            ent_scores = self.entity_attn(ent_context).squeeze(1)\n",
        "        else:\n",
        "            ent_scores = torch.zeros_like(sent_scores)\n",
        "\n",
        "        # Topic attention\n",
        "        if ('sentence', 'belongs_to', 'topic') in graph.canonical_etypes:\n",
        "            adj_st = self.dense_adj_from_edges(graph, ('sentence', 'belongs_to', 'topic'),\n",
        "                                               graph.num_nodes('sentence'), graph.num_nodes('topic'))\n",
        "            topic_context = adj_st @ topic_embs_proj\n",
        "            topic_scores = self.topic_attn(topic_context).squeeze(1)\n",
        "        else:\n",
        "            topic_scores = torch.zeros_like(sent_scores)\n",
        "\n",
        "        weights = F.softmax(self.raw_weights, dim=0)\n",
        "        alpha, beta, gamma = weights\n",
        "\n",
        "        final_scores = alpha * sent_scores + beta * ent_scores + gamma * topic_scores\n",
        "\n",
        "        if attention_breakdown:\n",
        "            return final_scores, {\n",
        "                'sentence': sent_scores.detach().cpu(),\n",
        "                'entity': ent_scores.detach().cpu(),\n",
        "                'topic': topic_scores.detach().cpu(),\n",
        "                'weights': weights.detach().cpu()\n",
        "            }\n",
        "\n",
        "        return final_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a4b6f21",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1a4b6f21"
      },
      "outputs": [],
      "source": [
        "# # Instantiate ML-GAP\n",
        "# mlgap_scorer = MultiLevelGraphAttentionPooling(embed_dim=64)\n",
        "# mlgap_scorer.eval()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     importance_scores = mlgap_scorer(\n",
        "#         sentence_embs=node_embeddings['sentence'],  # ‚úÖ use only sentence embeddings\n",
        "#         entity_embs=features['entity'],             # ‚úÖ entity features (300-dim)\n",
        "#         topic_embs=features['topic'],               # ‚úÖ topic features (300-dim)\n",
        "#         graph=graph\n",
        "#     )\n",
        "\n",
        "# # Select top-ranked sentences\n",
        "# top_k = 5\n",
        "# top_indices = torch.topk(importance_scores, k=top_k).indices\n",
        "# top_indices_sorted = sorted(top_indices.tolist())\n",
        "\n",
        "# # Generate final summary\n",
        "# summary_sentences = [sent_nodes[i] for i in top_indices_sorted]\n",
        "\n",
        "# import re\n",
        "# def clean_sentence(text):\n",
        "#     text = re.sub(r'@xmath\\d+', '', text)\n",
        "#     text = re.sub(r'@xcite', '', text)\n",
        "#     text = re.sub(r'\\s+', ' ', text)\n",
        "#     return text.strip()\n",
        "\n",
        "# cleaned_summary = [clean_sentence(sent) for sent in summary_sentences]\n",
        "\n",
        "# # Display output\n",
        "# for i, sent in enumerate(cleaned_summary, 1):\n",
        "#     print(f\"{i}. {sent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40876bbb",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "40876bbb"
      },
      "outputs": [],
      "source": [
        "# Install ROUGE scorer (only once needed)\n",
        "!pip install -q rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f242ad27",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f242ad27"
      },
      "outputs": [],
      "source": [
        "attention_data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bddddf5b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bddddf5b"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from statistics import mean\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def clean_sentence(text):\n",
        "    text = re.sub(r'@xmath\\d+', '', text)\n",
        "    text = re.sub(r'@xcite', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# ROUGE evaluator\n",
        "rouge_eval = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# ML-GAP model in eval mode\n",
        "mlgap_scorer = MultiLevelGraphAttentionPooling(embed_dim=64)\n",
        "mlgap_scorer.eval()\n",
        "\n",
        "# TF-IDF vectorizer for sentence features\n",
        "vectorizer = TfidfVectorizer(max_features=300)\n",
        "vectorizer.fit([\" \".join(preprocess_article(dataset[i]['article'])[0]) for i in range(50)])\n",
        "\n",
        "# Initialize metrics and storage\n",
        "rouge1_f, rouge2_f, rougel_f = [], [], []\n",
        "attention_data = []  # üîç for attention visualization\n",
        "num_docs_to_eval = 20\n",
        "\n",
        "# --- Evaluation loop ---\n",
        "for i in tqdm(range(num_docs_to_eval)):\n",
        "    article = dataset[i]['article']\n",
        "    reference = clean_sentence(dataset[i]['abstract'])\n",
        "\n",
        "    sents, ents = preprocess_article(article)\n",
        "    if len(sents) < 5:\n",
        "        continue\n",
        "\n",
        "    doc_topics, _ = topic_model.transform(sents)\n",
        "    g, sent_nodes, ent_nodes, topic_ids = build_graph_with_topics(sents, ents, precomputed_topics=doc_topics)\n",
        "\n",
        "    sent_feats_np = vectorizer.transform(sent_nodes).toarray()\n",
        "    if sent_feats_np.shape[1] < 300:\n",
        "        pad_width = 300 - sent_feats_np.shape[1]\n",
        "        sent_feats_np = np.pad(sent_feats_np, ((0, 0), (0, pad_width)), mode='constant')\n",
        "    sent_feats = torch.tensor(sent_feats_np, dtype=torch.float32)\n",
        "\n",
        "    ent_feats = torch.randn(g.num_nodes('entity'), 300)\n",
        "    topic_feats = torch.randn(g.num_nodes('topic'), 300)\n",
        "    feats = {'sentence': sent_feats, 'entity': ent_feats, 'topic': topic_feats}\n",
        "\n",
        "    # --- Forward pass with attention breakdown ---\n",
        "    with torch.no_grad():\n",
        "        node_embeds = model(g, feats)['sentence']\n",
        "        scores, parts = mlgap_scorer(\n",
        "            sentence_embs=node_embeds,\n",
        "            entity_embs=feats['entity'],\n",
        "            topic_embs=feats['topic'],\n",
        "            graph=g,\n",
        "            attention_breakdown=True\n",
        "        )\n",
        "\n",
        "        top_idx = torch.topk(scores, k=5).indices\n",
        "        top_idx_sorted = sorted(top_idx.tolist())\n",
        "        summary_sents = [clean_sentence(sent_nodes[j]) for j in top_idx_sorted]\n",
        "        generated_summary = \" \".join(summary_sents)\n",
        "\n",
        "        # Print per-sentence attention for summary\n",
        "        print(f\"\\nüîç Attention Breakdown for Document {i + 1}:\")\n",
        "        print(f\"Learned weights ‚Äî alpha: {parts['weights'][0]:.4f}, beta: {parts['weights'][1]:.4f}, gamma: {parts['weights'][2]:.4f}\")\n",
        "        for j in top_idx_sorted:\n",
        "            print(f\"- {clean_sentence(sent_nodes[j])}\")\n",
        "            print(f\"  Sentence: {parts['sentence'][j]:.4f} | Entity: {parts['entity'][j]:.4f} | Topic: {parts['topic'][j]:.4f}\")\n",
        "\n",
        "        # üîª Save full attention values for visualization\n",
        "        attention_data.append({\n",
        "            \"doc_id\": i,\n",
        "            \"sentence_texts\": [clean_sentence(s) for s in sent_nodes],\n",
        "            \"sentence_scores\": parts['sentence'].cpu().numpy().tolist(),\n",
        "            \"entity_scores\": parts['entity'].cpu().numpy().tolist(),\n",
        "            \"topic_scores\": parts['topic'].cpu().numpy().tolist(),\n",
        "            \"weights\": [x.item() for x in parts['weights']]\n",
        "        })\n",
        "\n",
        "    # --- ROUGE ---\n",
        "    results = rouge_eval.score(reference, generated_summary)\n",
        "    rouge1_f.append(results['rouge1'].fmeasure)\n",
        "    rouge2_f.append(results['rouge2'].fmeasure)\n",
        "    rougel_f.append(results['rougeL'].fmeasure)\n",
        "\n",
        "# --- Final report ---\n",
        "print(\"\\nüìä Average ROUGE F1 Scores over\", len(rouge1_f), \"documents:\")\n",
        "print(f\"ROUGE-1 F1: {mean(rouge1_f):.4f}\")\n",
        "print(f\"ROUGE-2 F1: {mean(rouge2_f):.4f}\")\n",
        "print(f\"ROUGE-L F1: {mean(rougel_f):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1866fc",
      "metadata": {
        "id": "3a1866fc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_attention_breakdown(attention_data, doc_index=0, sentence_texts=None, top_k=5, sort_by_score=False):\n",
        "    doc = attention_data[doc_index]\n",
        "\n",
        "    sentence_scores = np.array(doc['sentence_scores'])\n",
        "    entity_scores = np.array(doc['entity_scores'])\n",
        "    topic_scores = np.array(doc['topic_scores'])\n",
        "    final_scores = sentence_scores + entity_scores + topic_scores\n",
        "    num_sents = len(final_scores)\n",
        "\n",
        "    # Sorting if requested\n",
        "    if sort_by_score:\n",
        "        sort_order = np.argsort(final_scores)[::-1]\n",
        "        sentence_scores = sentence_scores[sort_order]\n",
        "        entity_scores = entity_scores[sort_order]\n",
        "        topic_scores = topic_scores[sort_order]\n",
        "        final_scores = final_scores[sort_order]\n",
        "        labels = [f\"S{i+1}\" for i in sort_order]\n",
        "        sorted_sentences = [sentence_texts[i] for i in sort_order] if sentence_texts else labels\n",
        "    else:\n",
        "        labels = [f\"S{i+1}\" for i in range(num_sents)]\n",
        "        sorted_sentences = sentence_texts if sentence_texts else labels\n",
        "\n",
        "    # üñ®Ô∏è Print top-k sentences\n",
        "    print(f\"\\nüìå Top-{top_k} Summary Sentences for Document {doc['doc_id'] + 1}:\\n\")\n",
        "    for rank in range(top_k):\n",
        "        print(f\"{rank+1}. {sorted_sentences[rank]}  (Score: {final_scores[rank]:.4f})\")\n",
        "\n",
        "    # üìä Plot\n",
        "    x = np.arange(num_sents)\n",
        "    width = 0.6\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.bar(x, sentence_scores, width, label='Sentence', color='skyblue')\n",
        "    ax.bar(x, entity_scores, width, bottom=sentence_scores, label='Entity', color='orange')\n",
        "    ax.bar(x, topic_scores, width, bottom=sentence_scores + entity_scores, label='Topic', color='lightgreen')\n",
        "\n",
        "    ax.set_ylabel('Attention Contribution')\n",
        "    ax.set_title(f'Attention Breakdown per Sentence (Doc {doc[\"doc_id\"] + 1})')\n",
        "\n",
        "    # ‚úÖ Only show every Nth tick label\n",
        "    tick_interval = max(1, num_sents // 30)\n",
        "    ax.set_xticks(x[::tick_interval])\n",
        "    ax.set_xticklabels([labels[i] for i in range(0, num_sents, tick_interval)], rotation=45, ha='right')\n",
        "\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9d5ef0",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3d9d5ef0"
      },
      "outputs": [],
      "source": [
        "plot_attention_breakdown(attention_data, doc_index=0, sentence_texts=sent_nodes, top_k=5, sort_by_score=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9651f7b",
      "metadata": {
        "collapsed": false,
        "id": "b9651f7b"
      },
      "source": [
        "# graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f6e2843",
      "metadata": {
        "id": "1f6e2843"
      },
      "source": [
        "## Heterogeneous Graph Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa68224",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5fa68224"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import dgl\n",
        "\n",
        "def visualize_graph_structure(g):\n",
        "    \"\"\"\n",
        "    Plot bar charts of node and edge type counts.\n",
        "    \"\"\"\n",
        "    node_counts = {ntype: g.num_nodes(ntype) for ntype in g.ntypes}\n",
        "    edge_counts = {str(etype): g.num_edges(etype) for etype in g.canonical_etypes}\n",
        "\n",
        "    # --- Node counts\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(node_counts.keys(), node_counts.values())\n",
        "    plt.title(\"Node Type Counts\")\n",
        "    plt.xlabel(\"Node Type\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Edge counts\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(edge_counts.keys(), edge_counts.values(), color='orange')\n",
        "    plt.title(\"Edge Type Counts\")\n",
        "    plt.xlabel(\"Edge Type\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call this after building your graph:\n",
        "visualize_graph_structure(graph)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9355abd3",
      "metadata": {
        "id": "9355abd3"
      },
      "source": [
        "## Mini Graph Visualization with NetworkX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c4e4a44",
      "metadata": {
        "id": "2c4e4a44"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw_mini_hetero_graph(g, sent_nodes, max_nodes=20, output_pdf_path='graph_output.pdf'):\n",
        "\n",
        "    G = nx.MultiDiGraph()\n",
        "\n",
        "    # Limit sentence nodes for clarity\n",
        "    node_ids = list(range(min(len(sent_nodes), max_nodes)))\n",
        "\n",
        "    for i in node_ids:\n",
        "        G.add_node(f\"Sent_{i}\", type='sentence')\n",
        "\n",
        "    if 'entity' in g.ntypes:\n",
        "        for i in range(min(g.num_nodes('entity'), 10)):\n",
        "            G.add_node(f\"Ent_{i}\", type='entity')\n",
        "\n",
        "    if 'topic' in g.ntypes:\n",
        "        for i in range(min(g.num_nodes('topic'), 5)):\n",
        "            G.add_node(f\"Topic_{i}\", type='topic')\n",
        "\n",
        "    # Add edges\n",
        "    for src, dst in zip(*g.edges(etype=('sentence', 'similar_to', 'sentence'))):\n",
        "        if src.item() in node_ids and dst.item() in node_ids:\n",
        "            G.add_edge(f\"Sent_{src.item()}\", f\"Sent_{dst.item()}\", label='similar_to')\n",
        "\n",
        "    if ('sentence', 'mentions', 'entity') in g.canonical_etypes:\n",
        "        src, dst = g.edges(etype=('sentence', 'mentions', 'entity'))\n",
        "        for s, e in zip(src, dst):\n",
        "            if s.item() in node_ids and e.item() < 10:\n",
        "                G.add_edge(f\"Sent_{s.item()}\", f\"Ent_{e.item()}\", label='mentions')\n",
        "\n",
        "    if ('sentence', 'belongs_to', 'topic') in g.canonical_etypes:\n",
        "        src, dst = g.edges(etype=('sentence', 'belongs_to', 'topic'))\n",
        "        for s, t in zip(src, dst):\n",
        "            if s.item() in node_ids and t.item() < 5:\n",
        "                G.add_edge(f\"Sent_{s.item()}\", f\"Topic_{t.item()}\", label='belongs_to')\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    pos = nx.spring_layout(G, seed=42, k=0.6)\n",
        "    node_colors = []\n",
        "    for n in G.nodes(data=True):\n",
        "        if n[1]['type'] == 'sentence':\n",
        "            node_colors.append('skyblue')\n",
        "        elif n[1]['type'] == 'entity':\n",
        "            node_colors.append('orange')\n",
        "        else:\n",
        "            node_colors.append('lightgreen')\n",
        "\n",
        "    # Draw the graph\n",
        "    nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=800, font_size=8, arrows=True)\n",
        "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
        "\n",
        "    # Title and appearance\n",
        "    plt.title(\"Mini Heterogeneous Graph\")\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the graph as a PDF file\n",
        "    plt.savefig(output_pdf_path, format='pdf')\n",
        "\n",
        "    # Show the plot (optional)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Graph saved to {output_pdf_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d081542",
      "metadata": {
        "id": "2d081542"
      },
      "source": [
        "## Attention Score Heatmap per Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fa6d0b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "05fa6d0b"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_attention_heatmap(attention_data, doc_index=0, top_k=10):\n",
        "    doc = attention_data[doc_index]\n",
        "    scores = np.stack([\n",
        "        doc[\"sentence_scores\"],\n",
        "        doc[\"entity_scores\"],\n",
        "        doc[\"topic_scores\"]\n",
        "    ])\n",
        "\n",
        "    scores = scores[:, :top_k]  # Limit to top-k for clarity\n",
        "    labels = [f\"S{i+1}\" for i in range(top_k)]\n",
        "    sources = ['Sentence', 'Entity', 'Topic']\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.heatmap(scores, annot=True, xticklabels=labels, yticklabels=sources, cmap='YlGnBu')\n",
        "    plt.title(f\"Attention Breakdown Heatmap (Doc {doc['doc_id'] + 1})\")\n",
        "    plt.xlabel(\"Sentence Index\")\n",
        "    plt.ylabel(\"Attention Source\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call with your saved attention_data\n",
        "plot_attention_heatmap(attention_data, doc_index=0, top_k=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51dac544",
      "metadata": {
        "id": "51dac544"
      },
      "source": [
        "## Combine with Summary Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18ae2749",
      "metadata": {
        "id": "18ae2749"
      },
      "outputs": [],
      "source": [
        "def print_summary_from_attention(attention_data, sent_nodes, doc_index=0, top_k=5):\n",
        "    doc = attention_data[doc_index]\n",
        "    sentence_scores = np.array(doc['sentence_scores']) + \\\n",
        "                      np.array(doc['entity_scores']) + \\\n",
        "                      np.array(doc['topic_scores'])\n",
        "\n",
        "    top_idx = np.argsort(sentence_scores)[::-1][:top_k]\n",
        "    print(f\"\\nüìå Top-{top_k} Summary Sentences for Document {doc['doc_id'] + 1}:\\n\")\n",
        "    for rank, idx in enumerate(top_idx):\n",
        "        print(f\"{rank + 1}. {sent_nodes[idx]}\")\n",
        "\n",
        "# Call after inference\n",
        "print_summary_from_attention(attention_data, sent_nodes, doc_index=0)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 676.790507,
      "end_time": "2025-07-13T18:06:18.653928",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-07-13T17:55:01.863421",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}